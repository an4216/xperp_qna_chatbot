# llm.py

# LangChain ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, FewShotChatMessagePromptTemplate
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

from langchain.vectorstores import FAISS
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.runnables import RunnableLambda, RunnableBranch, RunnablePassthrough, RunnableMap


# âœ… Ollamaìš© LLM
from langchain_community.chat_models import ChatOllama
# âœ… HuggingFace bge-m3 ì„ë² ë”©
from langchain_community.embeddings import HuggingFaceBgeEmbeddings

from config import answer_examples
import os
import time
import re

# =========================================
# í™˜ê²½ì„¤ì •
# =========================================
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
MODEL_LLM   = os.getenv("MODEL_LLM", "gemma3:latest")  # ollama pull gemma3:latest
TOP_K       = int(os.getenv("TOP_K", "4"))
VECTOR_DIR  = os.getenv("VECTOR_DIR", "vectorstore")
REFUSAL_MSG = "ì£„ì†¡í•©ë‹ˆë‹¤. ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ê·¼ê±°ë¥¼ ì°¾ì§€ ëª»í•´ ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´í™”í•˜ê±°ë‚˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”."
# ì„¸ì…˜ë³„ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ì†Œ
store = {}

# -------------------------------
# ìœ í‹¸: few-shot ì˜ˆì‹œì˜ 'ì¶œì²˜' ë¬¸êµ¬ ì œê±°
# -------------------------------
def sanitize_examples(examples: list[dict]) -> list[dict]:
    """
    Few-shot ì˜ˆì‹œì— í¬í•¨ëœ 'ì¶œì²˜/í˜ì´ì§€' í‘œê¸°(í˜•ì‹ ì˜ˆì‹œ)ë¥¼ ì œê±°í•´ì„œ,
    ì‹¤ì œ ì¸ìš©ì€ ë°˜ë“œì‹œ contextì˜ metadata(source/page)ë¡œë§Œ í•˜ë„ë¡ ë³´ì¡°í•œë‹¤.
    """
    sanitized = []
    for ex in examples:
        inp = ex.get("input", "")
        ans = ex.get("answer", "")

        # 1) 'âœ… ë§¤ë‰´ì–¼ ì°¸ì¡°:' ë¼ì¸ ì œê±°
        ans = re.sub(r'^\s*âœ…\s*ë§¤ë‰´ì–¼\s*ì°¸ì¡°:.*$', '', ans, flags=re.MULTILINE)

        # 2) ë³¸ë¬¸ ë‚´ ì„ì˜ ì¶œì²˜ ê´„í˜¸ ì œê±°: (ì¶œì²˜: ...í˜ì´ì§€)
        ans = re.sub(r'\(ì¶œì²˜:\s*[^)]+\)', '', ans)

        # 3) '...í˜ì´ì§€ ì°¸ì¡°' ë¥˜ ë¬¸êµ¬ ì œê±° (ì„ íƒì )
        ans = re.sub(r'[(ï¼ˆ]?\s*[^)\n]*ë§¤ë‰´ì–¼[^)\n]*\d+\s*í˜ì´ì§€\s*ì°¸ì¡°[)ï¼‰]?', '', ans)

        # 4) ì—¬ë¶„ ê³µë°± ì •ë¦¬
        ans = re.sub(r'\n{3,}', '\n\n', ans).strip()

        sanitized.append({"input": inp, "answer": ans})
    return sanitized

# 1. ì„¸ì…˜ë³„ ëŒ€í™” ì´ë ¥ ê°ì²´ ë°˜í™˜
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# âœ… bge-m3 ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
def get_embeddings():
    return HuggingFaceBgeEmbeddings(
        model_name="BAAI/bge-m3",
        encode_kwargs={"normalize_embeddings": True}  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì•ˆì •í™”
    )

# 2. ë¬¸ì„œ ë¡œë“œ + ë²¡í„°ìŠ¤í† ì–´ ìƒì„± + retriever ë°˜í™˜
def get_retriever():
    os.makedirs(VECTOR_DIR, exist_ok=True)

    # ì €ì¥ëœ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
    if os.path.exists(os.path.join(VECTOR_DIR, "index.faiss")):
        vectorstore = FAISS.load_local(
            VECTOR_DIR,
            get_embeddings(),
            allow_dangerous_deserialization=True
        )
        return vectorstore.as_retriever(search_kwargs={'k': TOP_K})

    # ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    embedding = get_embeddings()
    documents = []
    docs_dirs = ["docs/manual", "docs/qna"]

    for docs_dir in docs_dirs:
        if not os.path.isdir(docs_dir):
            continue
        for filename in os.listdir(docs_dir):
            file_path = os.path.join(docs_dir, filename)
            manual_name = os.path.splitext(filename)[0]

            if filename.endswith(".txt"):
                loader = TextLoader(file_path, encoding='utf-8')
                docs = loader.load()
                for doc in docs:
                    doc.metadata["source"] = manual_name
                documents.extend(docs)

            elif filename.endswith(".pdf"):
                loader = PyPDFLoader(file_path)
                pages = loader.load()
                for i, page in enumerate(pages):
                    page.metadata["source"] = manual_name
                    page.metadata["page"] = i + 1
                    # âœ… ì¶œì²˜ ì •ë³´ ì‚½ì… (ì‹¤ì œ ì¸ìš©ì€ ì—¬ê¸°ì„œë§Œ)
                    citation = f"\n\n(ì¶œì²˜: {manual_name} {i + 1}í˜ì´ì§€)"
                    page.page_content += citation
                    documents.append(page)

    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    split_docs = splitter.split_documents(documents)
    split_docs = [doc for doc in split_docs if len(doc.page_content.strip()) > 10]

    MAX_CHUNKS = 500
    if len(split_docs) > MAX_CHUNKS:
        split_docs = split_docs[:MAX_CHUNKS]

    vectorstore = FAISS.from_documents(split_docs, embedding)
    vectorstore.save_local(VECTOR_DIR)

    return vectorstore.as_retriever(search_kwargs={'k': TOP_K})

# 4. LLM(ì±—ë´‡) ì¸ìŠ¤í„´ìŠ¤ ìƒì„± â†’ Ollama
def get_llm():
    # í•„ìš” ì‹œ num_predict, temperature, keep_alive ë“± íŒŒë¼ë¯¸í„° ì¶”ê°€ ê°€ëŠ¥
    return ChatOllama(
        base_url=OLLAMA_HOST,
        model=MODEL_LLM,
        # ì•„ë˜ëŠ” ì„ íƒ: ì†ë„/ì•ˆì •ì„± íŠœë‹ ì˜ˆì‹œ
        temperature=0.0,
        # top_p=0.9,
        # num_predict=256,
        # num_ctx=4096,
        # keep_alive="30m",
    )

# 3. ëŒ€í™” ë§¥ë½ì„ ë°˜ì˜í•œ retriever ë°˜í™˜ (standalone question ë³€í™˜ + ë²¡í„°ê²€ìƒ‰)
def get_history_retriever():
    llm = get_llm()
    retriever = get_retriever()

    contextualize_q_system_prompt = (
        "Given a chat history and the latest user question "
        "which might reference context in the chat history, "
        "formulate a standalone question which can be understood "
        "without the chat history. Do NOT answer the question, "
        "just reformulate it if needed and otherwise return it as is."
    )

    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )

    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )
    return history_aware_retriever

# 6. RAG ì²´ì¸
def get_rag_chain():
    llm = get_llm()

    cleaned_examples = sanitize_examples(answer_examples)

    example_prompt = ChatPromptTemplate.from_messages([
        ("human", "{input}"),
        ("ai", "{answer}"),
    ])
    few_shot_prompt = FewShotChatMessagePromptTemplate(
        example_prompt=example_prompt,
        examples=cleaned_examples,
    )

    # ğŸ‘‰ í”„ë¡¬í”„íŠ¸ 'ì›ë¬¸ ê·¸ëŒ€ë¡œ' ìœ ì§€ (í˜•ë‹˜ì´ ì“°ë˜ system_prompt ê·¸ëŒ€ë¡œ)
    system_prompt = (
        "ë‹¹ì‹ ì€ Xperp í”„ë¡œê·¸ë¨ì— ëŒ€í•œ ì „ë¬¸ ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤.\n"
        "ì‚¬ìš©ìëŠ” Xperpì˜ ì‚¬ìš©ë²•, ê¸°ëŠ¥, ì˜¤ë¥˜ í•´ê²° ë“±ì— ëŒ€í•´ ì§ˆë¬¸í•©ë‹ˆë‹¤.\n"
        "ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ë‹¤ìŒ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê°€ì¥ ì •í™•í•˜ê³  ì‹¤ë¬´ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤:\n"
        "1) ì§ˆë¬¸(Q)ê³¼ ë‹µë³€(A), í‚¤ì›Œë“œ(T)ê°€ í¬í•¨ëœ QnA ë¬¸ì„œ\n"
        "2) PDF ë§¤ë‰´ì–¼ ë° ê¸°íƒ€ í…ìŠ¤íŠ¸ ì„¤ëª… ë¬¸ì„œ\n\n"
        "ë‹µë³€ êµ¬ì„± ë°©ì‹ (qna.txt ìš°ì„ ):\n"
        "- ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ qna.txt ë¬¸ì„œì— ì¡´ì¬í•˜ê±°ë‚˜ í‚¤ì›Œë“œë¥¼ ì°¸ê³ í•˜ì—¬ ìœ ì‚¬í•œ í•­ëª©ì´ ìˆë‹¤ë©´, í•´ë‹¹ A ë‚´ìš©ì„ ìš°ì„ ì ìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ ë‹µë³€ì˜ ë§¨ ì²˜ìŒì— ì œê³µí•©ë‹ˆë‹¤.\n"
        "- ì´í›„ PDF ë§¤ë‰´ì–¼ ë“± ê¸°íƒ€ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ë³´ì™„ ì„¤ëª…ì„ ì´ì–´ì„œ ì‘ì„±í•©ë‹ˆë‹¤.\n"
        "- ë¬¸ì„œì— ë”°ë¼ ì•„ë˜ í˜•ì‹ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ëˆëœ ë‹µë³€ì„ ê°€ë…ì„±ì„ ê³ ë ¤í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”:\n\n"
        "âœ… ì§ˆë¬¸ì— ëŒ€í•œ ì •ì‹ ë‹µë³€:\n"
        "- ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì˜ ê°œë…, ëª©ì , ë™ì‘ ì›ë¦¬ë¥¼ ìƒì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.\n"
        "- ì‹¤ë¬´ìê°€ ì˜¤í•´í•  ìˆ˜ ìˆëŠ” ì§€ì ì´ë‚˜ ìì£¼ ë¬»ëŠ” ìƒí™©ë„ í•¨ê»˜ ì•ˆë‚´í•©ë‹ˆë‹¤.\n\n"
        "âœ… ê°„ë‹¨ ìš”ì•½:\n"
        "- í•µì‹¬ ê°œë…ì„ 1~2ì¤„ ì´ë‚´ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.\n\n"
        "âœ… ì‚¬ìš©ë²• ì•ˆë‚´:\n"
        "- ë©”ë‰´ ê²½ë¡œ, ì„¤ì • ë°©ë²•, ì…ë ¥ ì ˆì°¨ë¥¼ ë¬¸ì„œì— ìˆëŠ” ë‚´ìš©ìœ¼ë¡œ ë‹¨ê³„ë³„ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
        "- ì…ë ¥ ì˜ˆì‹œë‚˜ í™”ë©´ ìœ„ì¹˜ ì •ë³´ë„ ê°€ëŠ¥í•œ ê²½ìš° í¬í•¨í•©ë‹ˆë‹¤.\n\n"
        "âœ… ìœ ì˜ì‚¬í•­:\n"
        "- ì‹¤ë¬´ ì¤‘ ìì£¼ ë°œìƒí•˜ëŠ” ì‹¤ìˆ˜ë‚˜ ì˜ˆì™¸ ìƒí™©, ê¸°ëŠ¥ ì œì•½ì‚¬í•­ ë“±ì„ êµ¬ì²´ì ìœ¼ë¡œ ê¸°ìˆ í•©ë‹ˆë‹¤.\n"
        "- ì‚¬ìš©ìê°€ ë†“ì¹˜ê¸° ì‰¬ìš´ ì¡°ê±´ì´ë‚˜ í™•ì¸ í•­ëª©ë„ í•¨ê»˜ ì œì‹œí•˜ì„¸ìš”.\n\n"
        "âœ… ì¶”ê°€ ì„¤ëª…ì´ í•„ìš”í•œ ê²½ìš°:\n"
        "- ìœ„ 1~4 í•­ëª© ì´ì™¸ì—ë„ ì‚¬ìš©ìê°€ ì‹¤ë¬´ì—ì„œ ê¶ê¸ˆí•´í•  ë§Œí•œ ë‚´ìš©ì„ ì˜ˆìƒí•˜ì—¬ ì¶”ê°€ ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”.\n"
        "- ì˜ˆ: ì—°ë™ëœ ê¸°ëŠ¥, ê´€ë ¨ëœ ë‹¤ë¥¸ ë©”ë‰´, ì„¤ì • ì˜í–¥ ë²”ìœ„, ì˜¤ë¥˜ ë©”ì‹œì§€ ë°œìƒ ì›ì¸ ë“±\n"
        "- ë°˜ë“œì‹œ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì‹¤ì œë¡œ ì—°ê´€ëœ ì •ë³´ë§Œ ì œì‹œí•˜ì„¸ìš”.\n\n"
        "âœ… ì˜ˆìƒ ì§ˆë¬¸:\n"
        "- ì‚¬ìš©ìê°€ ì´ì–´ì„œ ê¶ê¸ˆí•´í•  ìˆ˜ ìˆëŠ” ë‚´ìš©ì„ 1~3ê°œ ë¬¸ì¥ìœ¼ë¡œ ì œì‹œí•˜ì„¸ìš”.\n"
        "- qnaì™€ ë§¤ë‰´ì–¼ì—ì„œ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ë‚´ìš©ì„ ë°œì·Œí•˜ì—¬ ì œì‹œí•˜ì„¸ìš”.\n"
        "- qnaì™€ ë§¤ë‰´ì–¼ì— ë‚˜ì˜¨ ì˜ˆìƒì§ˆë¬¸ì´ ì¶”ê°€ë¡œ ì—†ë‹¤ë©´, ì˜ˆìƒì§ˆë¬¸ì„ ìƒëµí•´ì£¼ì„¸ìš”.\n\n"
        "âœ… ë§¤ë‰´ì–¼ ì°¸ì¡° ì¶œë ¥ ì§€ì¹¨:\n"
        "- ì•„ë˜ ì¡°ê±´ì„ ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•©ë‹ˆë‹¤:\n"
        "  1. ë°˜ë“œì‹œ 'context'ì˜ ë¬¸ì„œ metadata(source/page)ì—ì„œë§Œ ì¶œì²˜ë¥¼ ê°€ì ¸ì˜¤ì„¸ìš”.\n"
        "  2. few-shot ì˜ˆì‹œ(answer_examples) ì•ˆì˜ ì¶œì²˜/í˜ì´ì§€ í‘œê¸°ëŠ” ëª¨ë‘ ë¬´ì‹œí•˜ì„¸ìš”. (í˜•ì‹ ì˜ˆì‹œì¼ ë¿ ì‹¤ì œ ì¸ìš© ì•„ë‹˜)\n"
        "  3. ë¬¸ì„œëª…ì´ë‚˜ í˜ì´ì§€ë¥¼ ì„ì˜ë¡œ ì¶”ì¸¡í•˜ê±°ë‚˜ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.\n"
        "  4. ê° ì„¤ëª…ì´ ì–´ë–¤ ë¬¸ì„œì—ì„œ ìœ ë˜í–ˆëŠ”ì§€ ì‚¬ìš©ìì—ê²Œ ëª…í™•íˆ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.\n"
        "- ì‚¬ìš©ë²• ì•ˆë‚´ ë“±ì˜ ë‹µë³€ ë³¸ë¬¸ì—ì„œë„ ê´€ë ¨ ì„¤ëª… ëì— (ì¶œì²˜: ë¬¸ì„œëª… ní˜ì´ì§€) í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•´ ì£¼ì„¸ìš”.\n\n"
        "ì¶œë ¥ í˜•ì‹ ê·œì¹™(ë§¤ìš° ì¤‘ìš”):\n"
        "- ë°˜ë“œì‹œ Markdownì„ ì‚¬ìš©í•˜ì„¸ìš”.\n"
        "- ê° ì„¹ì…˜ ì œëª©(ì˜ˆ: 'âœ… ì§ˆë¬¸ì— ëŒ€í•œ ì •ì‹ ë‹µë³€:') ë’¤ì—ëŠ” ë¹ˆ ì¤„ 1ê°œë¥¼ ë‘ì„¸ìš”.\n"
        "- 'ì‚¬ìš©ë²• ì•ˆë‚´'ëŠ” ë²ˆí˜¸ ëª©ë¡(1., 2., 3., ...)ìœ¼ë¡œ, í•­ëª©ë§ˆë‹¤ ìƒˆ ì¤„ì—ì„œ ì‹œì‘í•˜ì„¸ìš”.\n"
        "- 'ìœ ì˜ì‚¬í•­', 'ì˜ˆìƒ ì§ˆë¬¸'ì€ ë¶ˆë¦¿ ëª©ë¡(- )ìœ¼ë¡œ, í•­ëª©ë§ˆë‹¤ ìƒˆ ì¤„ì—ì„œ ì‹œì‘í•˜ì„¸ìš”.\n"
        "- í•œ ì¤„ì— ì—¬ëŸ¬ í•­ëª©ì„ ì´ì–´ ì“°ì§€ ë§ˆì„¸ìš”. ê° í•­ëª©ì€ ë°˜ë“œì‹œ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤.\n\n"
        "{context}"
    )

    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        few_shot_prompt,
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])

    history_aware_retriever = get_history_retriever()
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

    # âœ… ëŸ°íƒ€ì„ ê°€ë“œ: ë¬¸ì„œê°€ 0ê°œë©´ ì¦‰ì‹œ ê±°ë¶€ë¬¸êµ¬ ë°˜í™˜, ì•„ë‹ˆë©´ QA ì²´ì¸ ì‹¤í–‰
    guard_input = RunnableMap({
        "input": lambda x: x["input"],
        "chat_history": lambda x: x.get("chat_history", []),
        "context": history_aware_retriever,   # â† retrieverê°€ ë¦¬ìŠ¤íŠ¸[Document] ë°˜í™˜
    })

    refuse = RunnableLambda(lambda _: {"answer": REFUSAL_MSG})

    guarded_chain = (
        guard_input
        | RunnableBranch(
            # ì¡°ê±´: context ë¹„ì—ˆìœ¼ë©´ ê±°ë¶€
            (lambda x: len(x.get("context", [])) == 0, refuse),
            # ê¸°ë³¸: QA ì‹¤í–‰
            question_answer_chain
        )
    )

    conversational_rag_chain = RunnableWithMessageHistory(
        guarded_chain,
        get_session_history,
        input_messages_key="input",
        history_messages_key="chat_history",
        output_messages_key="answer",
    ).pick('answer')

    return conversational_rag_chain

# 7. ìµœì¢… ë‹µë³€ ìƒì„± í•¨ìˆ˜
def get_ai_response(user_message):
    rag_chain = get_rag_chain()

    # âœ… 'input' í‚¤ë¡œ ì „ë‹¬
    stream = rag_chain.stream(
        {"input": user_message},
        config={"configurable": {"session_id": "abc123"}},
    )

    def timed_stream():
        start = time.perf_counter()
        for chunk in stream:
            yield chunk
        elapsed = time.perf_counter() - start
        yield f"\n\nâ± {elapsed:.2f}s"

    return timed_stream()
