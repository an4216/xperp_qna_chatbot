# LangChain ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, FewShotChatMessagePromptTemplate
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

from langchain.vectorstores import FAISS
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

from config import answer_examples
import os

# ì„¸ì…˜ë³„ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ì†Œ (ë©”ëª¨ë¦¬ dict)
store = {}

# 1. ì„¸ì…˜ë³„ ëŒ€í™” ì´ë ¥ ê°ì²´ ë°˜í™˜
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# 2. ë¬¸ì„œ ë¡œë“œ + ë²¡í„°ìŠ¤í† ì–´ ìƒì„± + retriever ë°˜í™˜
def get_retriever():
    from tqdm import tqdm

    # OpenAI ì„ë² ë”© ëª¨ë¸ ì •ì˜
    embedding = OpenAIEmbeddings(model='text-embedding-3-large')

    documents = []
    docs_dirs = ["docs/manual", "docs/qna"]  # ë§¤ë‰´ì–¼/QA í´ë” ë‘˜ ë‹¤ ë¡œë“œ

    # ë¬¸ì„œ ë¡œë“œ
    for docs_dir in docs_dirs:
        for filename in os.listdir(docs_dir):
            file_path = os.path.join(docs_dir, filename)

            if filename.endswith(".txt"):
                loader = TextLoader(file_path, encoding='utf-8')
                documents.extend(loader.load())

            elif filename.endswith(".pdf"):
                loader = PyPDFLoader(file_path)
                documents.extend(loader.load())

    # ë¬¸ì„œ ë¶„í• (ì²­í¬)
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    split_docs = splitter.split_documents(documents)

    # âœ… ë¹ˆ ë¬¸ì„œ ì œê±°
    split_docs = [doc for doc in split_docs if len(doc.page_content.strip()) > 10]

    # âœ… ìµœëŒ€ ì²­í¬ ê°œìˆ˜ ì œí•œ (ì˜ˆ: 1000ê°œ)
    MAX_CHUNKS = 500
    if len(split_docs) > MAX_CHUNKS:
        split_docs = split_docs[:MAX_CHUNKS]

    # âœ… FAISS ì €ì¥
    vectorstore = FAISS.from_documents(split_docs, embedding)

    # âœ… ê²€ìƒ‰ê¸° ìƒì„±
    retriever = vectorstore.as_retriever(search_kwargs={'k': 4})

    return retriever


# 3. ëŒ€í™” ë§¥ë½ì„ ë°˜ì˜í•œ retriever ë°˜í™˜ (standalone question ë³€í™˜ + ë²¡í„°ê²€ìƒ‰)
def get_history_retriever():
    llm = get_llm()
    retriever = get_retriever()

    # ëŒ€í™” ë§¥ë½+ìµœì‹  ì§ˆë¬¸ â†’ ì™„ì „í•œ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜ í”„ë¡¬í”„íŠ¸
    contextualize_q_system_prompt = (
        "Given a chat history and the latest user question "
        "which might reference context in the chat history, "
        "formulate a standalone question which can be understood "
        "without the chat history. Do NOT answer the question, "
        "just reformulate it if needed and otherwise return it as is."
    )

    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )

    # LangChain history aware retriever ìƒì„±
    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )
    return history_aware_retriever

# 4. LLM(ì±—ë´‡) ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ëª¨ë¸ ì„ íƒ ê°€ëŠ¥)
def get_llm(model='gpt-4o'):
    llm = ChatOpenAI(model=model)
    return llm

# 5. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì‚¬ì „ ê¸°ë°˜ìœ¼ë¡œ ì „ì²˜ë¦¬ (ì§ˆë¬¸ ë³€í™˜ ì²´ì¸)
def get_dictionary_chain():
    llm = get_llm()
    prompt = ChatPromptTemplate.from_template(f"""
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³´ê³ , ìš°ë¦¬ì˜ ì‚¬ì „ì„ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.
        ë§Œì•½ ë³€ê²½í•  í•„ìš”ê°€ ì—†ë‹¤ê³  íŒë‹¨ëœë‹¤ë©´, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.
        ê·¸ëŸ° ê²½ìš°ì—ëŠ” ì§ˆë¬¸ë§Œ ë¦¬í„´í•´ì£¼ì„¸ìš”

        ì§ˆë¬¸: {{question}}
    """)

    dictionary_chain = prompt | llm | StrOutputParser()

    return dictionary_chain

# 6. RAG(ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ) ì „ì²´ ì²´ì¸
def get_rag_chain():
    llm = get_llm()
    # Q/A ì˜ˆì‹œ(few-shot)ë¡œ LLMì˜ ë‹µë³€ í€„ë¦¬í‹° í–¥ìƒ
    example_prompt = ChatPromptTemplate.from_messages(
        [
            ("human", "{input}"),
            ("ai", "{answer}"),
        ]
    )
    few_shot_prompt = FewShotChatMessagePromptTemplate(
        example_prompt=example_prompt,
        examples=answer_examples,
    )

    # ì±—ë´‡ ì—­í•  ë° ë‹µë³€ ìŠ¤íƒ€ì¼ ì •ì˜(system í”„ë¡¬í”„íŠ¸)
    system_prompt = (
        "ë‹¹ì‹ ì€ Xperp í”„ë¡œê·¸ë¨ì— ëŒ€í•œ ì „ë¬¸ ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤.\n"
        "ì‚¬ìš©ìëŠ” Xperpì˜ ì‚¬ìš©ë²•, ê¸°ëŠ¥, ì˜¤ë¥˜ í•´ê²° ë“±ì— ëŒ€í•´ ì§ˆë¬¸í•©ë‹ˆë‹¤.\n"
        "ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ë¬¸ì„œì— í¬í•¨ëœ ì§ˆë¬¸(Q), ë‹µë³€(A), íƒœê·¸(T) ë˜ëŠ” PDF ë§¤ë‰´ì–¼ì„ ì°¸ê³ í•˜ì—¬,\n"
        "ê°€ì¥ ì •í™•í•˜ê³  ì‹¤ë¬´ì ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ì •ë³´ë¥¼ ë‹¤ìŒ í˜•ì‹ì— ë§ì¶° ì‘ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤:\n\n"

        "ì§ˆë¬¸ì— ëŒ€í•œ ì •ì‹ ë‹µë³€:\n"
        "- ì§ˆë¬¸ ì˜ë„ë¥¼ ëª…í™•íˆ íŒŒì•…í•˜ì—¬ ë°°ê²½ ì„¤ëª…ê³¼ í•¨ê»˜ ë‹µë³€í•©ë‹ˆë‹¤.\n"
        "- ê¸°ëŠ¥ì˜ ëª©ì , ì‚¬ìš© ì˜ˆì‹œ, ì ìš© ìƒí™© ë“±ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.\n"
        "- ì‚¬ìš©ì ì‹¤ìˆ˜ë‚˜ í˜¼ë™ì´ ì¦ì€ ë¶€ë¶„ì€ 'ì£¼ì˜ì‚¬í•­' í˜•íƒœë¡œ ì•ˆë‚´í•©ë‹ˆë‹¤.\n\n"

        "âœ… ê°„ë‹¨ ìš”ì•½:\n"
        "- ì§ˆë¬¸ì˜ í•µì‹¬ ê°œë…ì„ 1~2ì¤„ ìš”ì•½í•˜ì—¬ ì œê³µí•©ë‹ˆë‹¤.\n\n"

        "ğŸ”¹ ì‚¬ìš©ë²• ì•ˆë‚´:\n"
        "ğŸ“Œ 1. ë©”ë‰´ ê²½ë¡œ: ê¸°ëŠ¥ì´ ì†í•œ ì‹¤ì œ í™”ë©´ ê²½ë¡œë¥¼ ëª…ì‹œí•©ë‹ˆë‹¤.\n"
        "ğŸ“Œ 2. ì…ë ¥ ì ˆì°¨: í•­ëª©ë³„ ì…ë ¥ ìˆœì„œ ë° ì„¤ëª…ì„ ë²ˆí˜¸ìˆœìœ¼ë¡œ ë‚˜ì—´í•©ë‹ˆë‹¤.\n"
        "ğŸ“Œ 3. ì˜ˆì‹œ ì œê³µ: ê°€ëŠ¥í•œ ê²½ìš° ì…ë ¥ ì˜ˆì‹œë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n"
        "ğŸ“Œ 4. ê³„ì‚°ë°©ë²• êµ¬ë¶„: ì˜µì…˜(ë®ì–´ì“°ê¸°/ë”í•˜ê¸°/ë¹¼ê¸° ë“±)ì„ ëª…í™•íˆ ë¹„êµí•©ë‹ˆë‹¤.\n"
        "ğŸ“Œ 5. ì£¼ì˜ì‚¬í•­: ì‹¤ë¬´ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì˜¤ë¥˜ë‚˜ ìœ ì˜ì ì€ ê°•ì¡°í•´ í‘œì‹œí•©ë‹ˆë‹¤.\n\n"

        "ğŸ”¸ ì°¸ê³ ì‚¬í•­:\n"
        "- ì—‘ì…€ ì—…ë¡œë“œ, í•­ëª©ì½”ë“œ ì¡°íšŒ, ì„¸ëŒ€êµ¬ë¶„ ë“± ì‹¤ë¬´ì— í•„ìš”í•œ ë¶€ê°€ ì •ë³´ë¥¼ í•¨ê»˜ ì œê³µí•©ë‹ˆë‹¤.\n"
        "- ë¬¸ì„œì— ì •í™•í•œ ì •ë³´ê°€ ì—†ê±°ë‚˜ ì• ë§¤í•  ê²½ìš°, 'XPERPì— ê´€ë ¨ëœ ìƒë‹´ë§Œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.' ë¼ê³  ë‹µë³€í•©ë‹ˆë‹¤.\n"
        "- ì„¤ëª…ì€ ì¹œì ˆí•˜ì§€ë§Œ ê°„ê²°í•˜ê²Œ, ë¶ˆí•„ìš”í•œ ë§íˆ¬ë‚˜ ê³¼í•œ ë°˜ë³µì€ ì§€ì–‘í•©ë‹ˆë‹¤.\n\n"

        "{context}"
    )


    # QA í”„ë¡¬í”„íŠ¸(ì‹œìŠ¤í…œ + few-shot + íˆìŠ¤í† ë¦¬ + ì§ˆë¬¸)
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            few_shot_prompt,
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )

    # ëŒ€í™”í˜• retriever(standalone question â†’ ë²¡í„°ê²€ìƒ‰)
    history_aware_retriever = get_history_retriever()
    # LLM ë‹µë³€ ìƒì„± ì²´ì¸ (ë¬¸ì„œ context ì£¼ì…)
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    # Retrieval â†’ LLM QA ì²´ì¸
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    # ëŒ€í™” ì´ë ¥ Sessionë³„ë¡œ ê´€ë¦¬ (RunnableWithMessageHistory)
    conversational_rag_chain = RunnableWithMessageHistory(
        rag_chain,
        get_session_history,
        input_messages_key="input",
        history_messages_key="chat_history",
        output_messages_key="answer",
    ).pick('answer')

    return conversational_rag_chain

# 7. ìµœì¢… ë‹µë³€ ìƒì„± í•¨ìˆ˜ (ì§ˆë¬¸ â†’ ë‹µë³€)
def get_ai_response(user_message):
    # step 1: ì‚¬ì „ ê¸°ë°˜ ì§ˆë¬¸ ì „ì²˜ë¦¬
    dictionary_chain = get_dictionary_chain()
    # step 2: RAG ë¬¸ì„œê¸°ë°˜ QA ì²´ì¸
    rag_chain = get_rag_chain()
    # step 3: ì‚¬ì „ ì²´ì¸ ê²°ê³¼ë¥¼ rag_chainì˜ inputìœ¼ë¡œ ì—°ê²°
    tax_chain = {"input": dictionary_chain} | rag_chain

    # step 4: ë‹µë³€ ìƒì„± (streaming)
    ai_response = tax_chain.stream(
        {
            "question": user_message
        },
        config={
            "configurable": {"session_id": "abc123"}
        },
    )

    return ai_response
